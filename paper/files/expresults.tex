

\section{Experiments}

\subsection{Map prediction}

The first step in making better decisions about the future is to predict the future.
We learn the prior of over indoor environments and use it to predict the unknown
parts in the map. Some of the examples of inputs, mask and predicted pairs are
given in Figure~\ref{fig:map-prediction}.

\subsubsection{Generation of training data}

We collect indoor maps from various datasets ~\cite{howard2003radish}.
We run an occupancy grid algorithm, specifically Omnimapper~\cite{Trevor14ICRA} and generate maps. 
The maps are saved and a mask is generated by running morphological operations
on the known part of the map.
The newly explored known part of the maps is masked out and the map and mask
pair are fed into the image-inpainting training algorithm.
The image-inpainting algorithm is only fine-tuned on this new dataset.
The result of map-prediction is shown in Figure~\ref{fig:map-prediction}.


\subsection{Real World Experiments}

We evaluate our system performing a series of experiments incorporated real-world robot operation within environments over Atkinson and Jacobs Halls on the campus of the University of California, San Diego.  This large environments contains obstacles that are typical of office buildings.

Each robot generates their own map using our mapping framework OmniMapper~\cite{Trevor14ICRA}. An initial pose estimate is used to speed up the convergence of the localization routine. While the localization routine is converged on a pose hypothesis, the corresponding RSSI measurements are computed into a GP model using  libGP~\cite{BlumLibGP}. 

We model RSSI using Gaussian Process (GP) regression, a standard method for spatial field modeling~\cite{Rasmussen2006}.
The GP approximates measurements at different locations with Gaussian functions based on a prior mean and covariance function $k(\cdot,\cdot)$, also known as the kernel.
When a sample $y$ at location ${\bf x}$ is added to the GP, the Gaussians at locations ${\bf x}'$ are updated based on the chosen kernel, where ${\bf x,x}' \in \mathbb{R}^2$.

For each possible sampling location ${\bf x}_*$ the GP has a predictive mean and predictive variance~\cite{Rasmussen2006}:

\begin{align}
 \mu_{y_*}({\bf x}, {\bf x}_*, y) &=  k({\bf x}_*,{\bf x}) k({\bf x},{\bf x}) y\\
 \sigma^2_{y_*}({\bf x}, {\bf x}_*) &= k({\bf x}_*,{\bf x}_*) - k({\bf x}_*,{\bf x}) k({\bf x},{\bf x})^{-1} k({\bf x},{\bf x}_*)
\end{align}
% 
where ${\bf x}$ and $y$ are training locations and measurements, ${\bf x_*}$ are test locations,
and $k(\cdot,\cdot)$ is the kernel, i.e. the covariance function. The predictive variance is especially useful in adaptive sampling, because it gives an estimate of the confidence in the predicted mean, and robots can choose to sample in locations with high model uncertainty.

